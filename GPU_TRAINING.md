# ğŸš€ GPU è®­ç»ƒæŒ‡å—

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ç›´æ¥è¿è¡Œæ‰¹å¤„ç†ï¼ˆæ¨èï¼‰

```bash
# Windows CMD æˆ– PowerShell
train_gpu.bat

# æˆ–æŒ‡å®šè®­ç»ƒè½®æ•°
train_gpu.bat --max-epochs 50
```

### æ–¹å¼ 2: å‘½ä»¤è¡Œè¿è¡Œ

```bash
python -m src.training.train \
    --batch-size 16 \
    --hidden-dim 256 \
    --n-layers 3 \
    --max-epochs 100
```

### æ–¹å¼ 3: Python è„šæœ¬è¿è¡Œ

```python
python train_gpu.py
```

## GPU ä¼˜åŒ–é…ç½®

å·²è‡ªåŠ¨ä¸º GPU ä¼˜åŒ–ä»¥ä¸‹å‚æ•°ï¼š

| å‚æ•° | CPU | GPU | è¯´æ˜ |
|------|-----|-----|------|
| **Batch Size** | 4 | 16 | GPU å¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡ |
| **éšè—ç»´åº¦** | 512 | 256 | å‡å°‘æ˜¾å­˜å ç”¨åŒæ—¶ä¿æŒæ€§èƒ½ |
| **Transformer å±‚æ•°** | 6 | 3 | GPU ä¼˜åŒ–åé€šå¸¸ 3-4 å±‚è¶³å¤Ÿ |
| **æ¢¯åº¦ç´¯ç§¯æ­¥æ•°** | 8 | 2 | GPU å‡å°‘ç´¯ç§¯æ­¥æ•°ä»¥åŠ å¿«æ”¶æ•› |
| **æ··åˆç²¾åº¦** | å¦ | æ˜¯ | GPU å¯ç”¨ FP16 åŠ é€Ÿ |

## æ€§èƒ½ä¼˜åŒ–

### 1. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆè‡ªåŠ¨å¯ç”¨ï¼‰

âœ“ å¯ç”¨ FP16 è®¡ç®—ï¼Œå‡å°‘æ˜¾å­˜å ç”¨ 50%
âœ“ æå‡è®­ç»ƒé€Ÿåº¦ 20-30%
âœ“ ä¿æŒç²¾åº¦ä¸æŸå¤±

### 2. æ¢¯åº¦æ£€æŸ¥ç‚¹

ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘æ˜¾å­˜å ç”¨ï¼ˆéœ€è¦æ—¶æ‰‹åŠ¨å¯ç”¨ï¼‰

### 3. æ˜¾å­˜ç®¡ç†

è‡ªåŠ¨æ¸…ç†ä¸éœ€è¦çš„å˜é‡ï¼Œé¿å…æ˜¾å­˜æ³„æ¼

## å¸¸è§é—®é¢˜

### Q: æ˜¾å­˜ä¸è¶³é”™è¯¯

A: å°è¯•ä»¥ä¸‹æ–¹æ¡ˆï¼ˆæŒ‰é¡ºåºï¼‰ï¼š

```bash
# 1. å‡å° batch size
python -m src.training.train --batch-size 8

# 2. å‡å°‘éšè—ç»´åº¦
python -m src.training.train --hidden-dim 128

# 3. å‡å°‘å±‚æ•°
python -m src.training.train --n-layers 2

# 4. ä½¿ç”¨ CPUï¼ˆæœ€åé€‰é¡¹ï¼‰
python -m src.training.train --device cpu
```

### Q: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

A: å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

1. **æœªä½¿ç”¨ GPU**
   ```bash
   # æ£€æŸ¥ GPU
   python -c "import torch; print('GPU:', torch.cuda.is_available())"
   ```

2. **æ··åˆç²¾åº¦æœªå¯ç”¨**
   ```bash
   # éªŒè¯æ··åˆç²¾åº¦å¼€å¯
   python -m src.training.train  # é»˜è®¤å¯ç”¨
   ```

3. **Batch size å¤ªå°**
   ```bash
   # å¢åŠ  batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
   python -m src.training.train --batch-size 32
   ```

### Q: å¦‚ä½•ç›‘æŸ¥è®­ç»ƒè¿›åº¦

A: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„ä»¥ä¸‹æŒ‡æ ‡ï¼š

- **Loss**: åº”è¯¥é€æ¸ä¸‹é™
- **LR (å­¦ä¹ ç‡)**: åº”è¯¥åœ¨é¢„çƒ­åç¨³å®š
- **ååé‡**: æ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°

### Q: å¦‚ä½•ä¸­æ–­è®­ç»ƒå¹¶æ¢å¤

A: è®­ç»ƒä¸­æŒ‰ `Ctrl+C` ä¸­æ–­

æ¢å¤è®­ç»ƒï¼š
```bash
python -m src.training.train --resume models/checkpoint-latest.pt
```

## ç›‘æŸ¥ GPU ä½¿ç”¨

### å®æ—¶ç›‘æŸ¥æ˜¾å­˜

**Windows**:
```bash
# NVIDIA GPU ç›‘æŸ¥
nvidia-smi -l 1  # æ¯ç§’åˆ·æ–°ä¸€æ¬¡
```

**Linux**:
```bash
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯

```bash
# æ‰€æœ‰ GPU ä¿¡æ¯
nvidia-smi -q

# è¿›ç¨‹åˆ—è¡¨
nvidia-smi pmon -c 1
```

## æ€§èƒ½åŸºå‡†

åŸºäºä¸åŒç¡¬ä»¶çš„é¢„æœŸè®­ç»ƒé€Ÿåº¦ï¼š

| GPU | Batch=16 | å†…å­˜å ç”¨ | ååé‡ |
|-----|----------|----------|--------|
| RTX 4090 | ~5 ms | ~10GB | 3200 æ ·æœ¬/ç§’ |
| RTX 4080 | ~8 ms | ~15GB | 2000 æ ·æœ¬/ç§’ |
| RTX 4070 | ~12 ms | ~20GB | 1300 æ ·æœ¬/ç§’ |
| RTX 3090 | ~15 ms | ~18GB | 1000 æ ·æœ¬/ç§’ |
| RTX 3080 | ~18 ms | ~22GB | 800 æ ·æœ¬/ç§’ |

## é«˜çº§é€‰é¡¹

### ä½¿ç”¨ WandB è·Ÿè¸ªå®éªŒ

```bash
python -m src.training.train --use-wandb
```

éœ€è¦å…ˆæ³¨å†Œ WandB è´¦æˆ·ï¼šhttps://wandb.ai

### ç¦ç”¨æ··åˆç²¾åº¦ï¼ˆè°ƒè¯•ï¼‰

```bash
python -m src.training.train --no-mixed-precision
```

### è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦

ç¼–è¾‘ `src/training/train.py` ä¸­çš„ `_create_scheduler()` æ–¹æ³•

## æ•…éšœæ’æŸ¥

### GPU ä¸è¢«æ£€æµ‹åˆ°

1. ç¡®ä¿ NVIDIA é©±åŠ¨å·²å®‰è£…ï¼š
   ```bash
   nvidia-smi
   ```

2. æ£€æŸ¥ PyTorch CUDA æ”¯æŒï¼š
   ```bash
   python -c "import torch; print(torch.cuda.is_available())"
   ```

3. å¦‚æœéƒ½æ­£å¸¸ä½†ä»æœªæ£€æµ‹åˆ°ï¼Œé‡æ–°å®‰è£… PyTorchï¼š
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```

### OOM é”™è¯¯æŒç»­å‡ºç°

1. æ¸…ç©º CUDA ç¼“å­˜ï¼š
   ```bash
   python -c "import torch; torch.cuda.empty_cache()"
   ```

2. å…³é—­å…¶ä»– GPU åº”ç”¨ç¨‹åº

3. ä½¿ç”¨ CPU è®­ç»ƒä¸´æ—¶è°ƒè¯•ä»£ç 

## ä¼˜åŒ–å»ºè®®

### å¯¹äºå° GPUï¼ˆ<10GB æ˜¾å­˜ï¼‰

```bash
train_gpu.bat --batch-size 8 --hidden-dim 128 --n-layers 2
```

### å¯¹äºå¤§ GPUï¼ˆ>20GB æ˜¾å­˜ï¼‰

```bash
train_gpu.bat --batch-size 32 --hidden-dim 512 --n-layers 6
```

### å¯¹äºå¤š GPU è®­ç»ƒï¼ˆéœ€è¦æ‰‹åŠ¨é…ç½®ï¼‰

ç¼–è¾‘ `src/training/train.py`ï¼Œä½¿ç”¨ `torch.nn.DataParallel` æˆ– `DistributedDataParallel`

## å‚è€ƒèµ„æº

- [PyTorch CUDA æ–‡æ¡£](https://pytorch.org/docs/stable/cuda.html)
- [æ··åˆç²¾åº¦è®­ç»ƒæŒ‡å—](https://pytorch.org/docs/stable/amp.html)
- [æ˜¾å­˜ä¼˜åŒ–æŠ€å·§](https://pytorch.org/docs/stable/notes/cuda.html)
